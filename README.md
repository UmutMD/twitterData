# twitterData

Current Problems:

-Delimiter not working(?)
-Or whitespace (?)

How to seperate Context column into words ?



Solved Problems

Hadoop Inst.
Spark Inst.


Proposed solution:

Example DF looks like :  Root: Created at:
                         Element: Friday 22.22.2022
                         Root: Context:
                         Element: "this is a tweet"
                         
                         
After this seperation, counting the same words occured in the same day will give us the trending words in that day.
